{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import import_ipynb\n",
    "sys.path.append(os.pardir)\n",
    "from DataSet.mnist import load_mnist\n",
    "# from Ch2_3.handWrittenDigit import get_data, init_network, predict, img_show\n",
    "# from Ch2_3.activation_function import softmax, sigmoid\n",
    "# from Ch4.load_error import cross_entropy_error\n",
    "from Ch5.layer_naive import Affine, ReLu, SoftmaxWithLoss, Sigmoid\n",
    "from Ch6.training_tech import AdaGrad, SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import S\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self,input_dim= (1,28,28),\\\n",
    "        conv_param={'filter_num':30, 'filter_size':5,\\\n",
    "                        'pad':0,'stride':1},\\\n",
    "        hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        \n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad)/\\\n",
    "                            filter_stride + 1\n",
    "        pool_output_size = int(filter_num* (conv_output_size/2)*(conv_output_size/2))\n",
    "\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std*\\\n",
    "                            np.random.randn(filter_num, input_dim[0],\\\n",
    "                                        filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std*\\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std*\\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "    \n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\\\n",
    "                                            self.params['b1'],\\\n",
    "                                            conv_param['stride'],\\\n",
    "                                            conv_param['pad'])\n",
    "        self.layers['Relu1'] = ReLu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        self.layers['Relu2'] = ReLu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y,t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # save result\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        # optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "        #                         'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        optimizer_class_dict = {'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300094793759105\n",
      "=== epoch:1, train acc:0.104, test acc:0.095 ===\n",
      "train loss:2.297155181540681\n",
      "train loss:2.2944820557108785\n",
      "train loss:2.2877143548781658\n",
      "train loss:2.278680854232502\n",
      "train loss:2.274859698575245\n",
      "train loss:2.2633014637902784\n",
      "train loss:2.2469179374421335\n",
      "train loss:2.2241702655325044\n",
      "train loss:2.19032178639862\n",
      "train loss:2.170285845650708\n",
      "train loss:2.1297941140315735\n",
      "train loss:2.1300294313485693\n",
      "train loss:2.0154488633724177\n",
      "train loss:1.9789301615520956\n",
      "train loss:1.926391436545854\n",
      "train loss:1.8757399299613624\n",
      "train loss:1.913739106524728\n",
      "train loss:1.7914688912624879\n",
      "train loss:1.6542396999344615\n",
      "train loss:1.55389211346823\n",
      "train loss:1.5301858524323066\n",
      "train loss:1.4473465224196889\n",
      "train loss:1.3384486309233645\n",
      "train loss:1.249720825776571\n",
      "train loss:1.1708601239066214\n",
      "train loss:1.2067051023896957\n",
      "train loss:1.0548201136686066\n",
      "train loss:1.0322300615271147\n",
      "train loss:0.989294284386916\n",
      "train loss:0.989644360451334\n",
      "train loss:0.845172939516073\n",
      "train loss:0.7647699449119365\n",
      "train loss:0.7432802443092185\n",
      "train loss:0.733542703807836\n",
      "train loss:0.7361604857800628\n",
      "train loss:0.7287249986576341\n",
      "train loss:0.7024409919386758\n",
      "train loss:0.6174175755059601\n",
      "train loss:0.777087139651163\n",
      "train loss:0.8367234647598987\n",
      "train loss:0.6347263398053514\n",
      "train loss:0.6808926972790359\n",
      "train loss:0.5058844348882535\n",
      "train loss:0.6411826764763014\n",
      "train loss:0.6290492171701813\n",
      "train loss:0.6131538942453773\n",
      "train loss:0.6776731015432844\n",
      "train loss:0.5122401179614139\n",
      "train loss:0.5013419769117984\n",
      "train loss:0.5753148248523927\n",
      "=== epoch:2, train acc:0.822, test acc:0.81 ===\n",
      "train loss:0.41766146388029546\n",
      "train loss:0.5530525352473432\n",
      "train loss:0.5983820731760083\n",
      "train loss:0.4874724693280433\n",
      "train loss:0.2989705147221489\n",
      "train loss:0.469643535995054\n",
      "train loss:0.4618020577530786\n",
      "train loss:0.34499293396030045\n",
      "train loss:0.44751091631076273\n",
      "train loss:0.6676311680809904\n",
      "train loss:0.6132801501942794\n",
      "train loss:0.65284795787525\n",
      "train loss:0.4175448471312727\n",
      "train loss:0.3335532395377615\n",
      "train loss:0.35683125000601856\n",
      "train loss:0.284413616343779\n",
      "train loss:0.6289121578403575\n",
      "train loss:0.31192057569472276\n",
      "train loss:0.5549239134986171\n",
      "train loss:0.5625419045362848\n",
      "train loss:0.41928448242569355\n",
      "train loss:0.2596297535082347\n",
      "train loss:0.44502857692283226\n",
      "train loss:0.32813849321580785\n",
      "train loss:0.573700317492415\n",
      "train loss:0.37688925654752886\n",
      "train loss:0.34569678444333357\n",
      "train loss:0.39260861417758897\n",
      "train loss:0.35903943798359145\n",
      "train loss:0.3780561525894744\n",
      "train loss:0.41182586729688064\n",
      "train loss:0.28065069248926383\n",
      "train loss:0.4398455461498996\n",
      "train loss:0.3168132577057257\n",
      "train loss:0.3031089833072828\n",
      "train loss:0.3082860779437352\n",
      "train loss:0.30670237073836776\n",
      "train loss:0.23415290379559076\n",
      "train loss:0.5348026617918036\n",
      "train loss:0.27961761412302516\n",
      "train loss:0.41975641667405844\n",
      "train loss:0.23211937610112854\n",
      "train loss:0.43785621574054806\n",
      "train loss:0.3527843977147779\n",
      "train loss:0.3092453022422743\n",
      "train loss:0.2066592201134948\n",
      "train loss:0.22657155618405642\n",
      "train loss:0.38239361164238445\n",
      "train loss:0.32511288947502065\n",
      "train loss:0.3906330212415676\n",
      "=== epoch:3, train acc:0.872, test acc:0.872 ===\n",
      "train loss:0.28066043249852796\n",
      "train loss:0.3076823269559448\n",
      "train loss:0.3445427325811755\n",
      "train loss:0.3655998998440729\n",
      "train loss:0.44029364197868115\n",
      "train loss:0.35426853408254827\n",
      "train loss:0.38661774775835406\n",
      "train loss:0.39174673492087225\n",
      "train loss:0.2315025517200133\n",
      "train loss:0.18910907803081733\n",
      "train loss:0.28000743457886645\n",
      "train loss:0.33083251846353984\n",
      "train loss:0.3702622829133879\n",
      "train loss:0.17407191048748213\n",
      "train loss:0.20103583054907417\n",
      "train loss:0.2616222651646748\n",
      "train loss:0.3442937911026372\n",
      "train loss:0.1757260596779949\n",
      "train loss:0.4040696578594231\n",
      "train loss:0.3767157018526773\n",
      "train loss:0.25386268198580586\n",
      "train loss:0.18249756370339504\n",
      "train loss:0.34376755012593113\n",
      "train loss:0.28663441367753156\n",
      "train loss:0.337999709350914\n",
      "train loss:0.2533536971352643\n",
      "train loss:0.2258112061208683\n",
      "train loss:0.22972571752666845\n",
      "train loss:0.3706715019210947\n",
      "train loss:0.40214907987189774\n",
      "train loss:0.32843217217261433\n",
      "train loss:0.3736521122039893\n",
      "train loss:0.3380706500965643\n",
      "train loss:0.35371384198188494\n",
      "train loss:0.37433138980079694\n",
      "train loss:0.3150000221129351\n",
      "train loss:0.38870840711515764\n",
      "train loss:0.26039690000233273\n",
      "train loss:0.251342912145446\n",
      "train loss:0.32169018879322964\n",
      "train loss:0.14093574226185818\n",
      "train loss:0.15252312392538742\n",
      "train loss:0.19646070389333034\n",
      "train loss:0.26494316277263597\n",
      "train loss:0.2021436093003689\n",
      "train loss:0.2652533319792387\n",
      "train loss:0.4754312497136774\n",
      "train loss:0.26562737200481107\n",
      "train loss:0.2967085277977599\n",
      "train loss:0.3243095206741978\n",
      "=== epoch:4, train acc:0.903, test acc:0.891 ===\n",
      "train loss:0.256606698703961\n",
      "train loss:0.19007821465746855\n",
      "train loss:0.2333950451221071\n",
      "train loss:0.31104969058823306\n",
      "train loss:0.11655581781558469\n",
      "train loss:0.30721411776803564\n",
      "train loss:0.3400392276793942\n",
      "train loss:0.14496909470509012\n",
      "train loss:0.3935848657369159\n",
      "train loss:0.21906916810369712\n",
      "train loss:0.1937226627969601\n",
      "train loss:0.23774345865854474\n",
      "train loss:0.2572028299964553\n",
      "train loss:0.16435634419318945\n",
      "train loss:0.1812841556961811\n",
      "train loss:0.32379280701182717\n",
      "train loss:0.14409513543246857\n",
      "train loss:0.15923359910078108\n",
      "train loss:0.24177399565731733\n",
      "train loss:0.2654883110535079\n",
      "train loss:0.13359556669442096\n",
      "train loss:0.14164010341909772\n",
      "train loss:0.15901062400334176\n",
      "train loss:0.12847253835896497\n",
      "train loss:0.25951013140432183\n",
      "train loss:0.30016994266647373\n",
      "train loss:0.31062820823624027\n",
      "train loss:0.35073957195826333\n",
      "train loss:0.2224746761606805\n",
      "train loss:0.11269279246425651\n",
      "train loss:0.2801464117321739\n",
      "train loss:0.15815913266096804\n",
      "train loss:0.25135514835671857\n",
      "train loss:0.2385363582053154\n",
      "train loss:0.14244354921318458\n",
      "train loss:0.23089632900938067\n",
      "train loss:0.24152660934355247\n",
      "train loss:0.22574284945001188\n",
      "train loss:0.17523541445142998\n",
      "train loss:0.196009357611738\n",
      "train loss:0.22282521661219068\n",
      "train loss:0.16366085854845625\n",
      "train loss:0.34849352650065923\n",
      "train loss:0.29221006315901765\n",
      "train loss:0.24218163802363832\n",
      "train loss:0.11758196127270219\n",
      "train loss:0.2570340382806202\n",
      "train loss:0.20168238200393865\n",
      "train loss:0.19283752328027748\n",
      "train loss:0.23783497554362057\n",
      "=== epoch:5, train acc:0.894, test acc:0.889 ===\n",
      "train loss:0.16286800248828864\n",
      "train loss:0.22691610262284292\n",
      "train loss:0.23437020800538136\n",
      "train loss:0.29330235651419573\n",
      "train loss:0.1650352316884137\n",
      "train loss:0.24986518440754918\n",
      "train loss:0.16263249441900537\n",
      "train loss:0.2596700862883931\n",
      "train loss:0.2537787936939654\n",
      "train loss:0.1939994698944225\n",
      "train loss:0.3804422332680192\n",
      "train loss:0.13637908666984344\n",
      "train loss:0.2676932630351254\n",
      "train loss:0.21791553193383514\n",
      "train loss:0.1274840154339363\n",
      "train loss:0.2089231909907133\n",
      "train loss:0.22955221575589474\n",
      "train loss:0.1454521119657213\n",
      "train loss:0.3105177763720845\n",
      "train loss:0.08786174176519122\n",
      "train loss:0.1772368965248587\n",
      "train loss:0.09783110657034597\n",
      "train loss:0.1764595831782532\n",
      "train loss:0.15924108330555845\n",
      "train loss:0.2801109950292355\n",
      "train loss:0.23670441974454245\n",
      "train loss:0.16116126364565989\n",
      "train loss:0.31331032171903567\n",
      "train loss:0.22959496914983762\n",
      "train loss:0.14767165332745574\n",
      "train loss:0.2811602358545215\n",
      "train loss:0.17948193687590738\n",
      "train loss:0.22823127564216766\n",
      "train loss:0.12882002074331037\n",
      "train loss:0.20246835421657366\n",
      "train loss:0.28488491098557267\n",
      "train loss:0.19378868477603597\n",
      "train loss:0.28867279461588424\n",
      "train loss:0.21371318592376135\n",
      "train loss:0.21533478746396637\n",
      "train loss:0.1500374419084193\n",
      "train loss:0.3604333510125801\n",
      "train loss:0.3505769546540058\n",
      "train loss:0.35148819787967756\n",
      "train loss:0.11528013338813002\n",
      "train loss:0.2766008777671776\n",
      "train loss:0.24576400318345581\n",
      "train loss:0.2048857529846718\n",
      "train loss:0.16609140448240217\n",
      "train loss:0.10931577513593722\n",
      "=== epoch:6, train acc:0.927, test acc:0.903 ===\n",
      "train loss:0.3660221719370503\n",
      "train loss:0.2653236201459918\n",
      "train loss:0.1984627088685328\n",
      "train loss:0.288009940094168\n",
      "train loss:0.13512803210395125\n",
      "train loss:0.2710830793816043\n",
      "train loss:0.16645145955156282\n",
      "train loss:0.18154987656127125\n",
      "train loss:0.1670897108122292\n",
      "train loss:0.1860915737102162\n",
      "train loss:0.17852555857780245\n",
      "train loss:0.10730701929936172\n",
      "train loss:0.22428281722647603\n",
      "train loss:0.10985877771553763\n",
      "train loss:0.13304449733381404\n",
      "train loss:0.24021160947481146\n",
      "train loss:0.1534642512865561\n",
      "train loss:0.21698038447388465\n",
      "train loss:0.14435526905478466\n",
      "train loss:0.15458758372601855\n",
      "train loss:0.1368240597903356\n",
      "train loss:0.11898914067480813\n",
      "train loss:0.09497985125998401\n",
      "train loss:0.2462760577786392\n",
      "train loss:0.15684039621462484\n",
      "train loss:0.16300691982696244\n",
      "train loss:0.18349498191680502\n",
      "train loss:0.20845654562423588\n",
      "train loss:0.08775961888848427\n",
      "train loss:0.17081198300045874\n",
      "train loss:0.20523464818234616\n",
      "train loss:0.14215654937814814\n",
      "train loss:0.16548025757172474\n",
      "train loss:0.21515274657043773\n",
      "train loss:0.16912660037801228\n",
      "train loss:0.15352315722768514\n",
      "train loss:0.15043260697215688\n",
      "train loss:0.14996061957263856\n",
      "train loss:0.13069392223197832\n",
      "train loss:0.19632479745093234\n",
      "train loss:0.10076796852502369\n",
      "train loss:0.22182412125153408\n",
      "train loss:0.09309653005123085\n",
      "train loss:0.17542243210174824\n",
      "train loss:0.14652284189169365\n",
      "train loss:0.17683709294426703\n",
      "train loss:0.12689575680285692\n",
      "train loss:0.1021190891945062\n",
      "train loss:0.1487933680926719\n",
      "train loss:0.18563518075092125\n",
      "=== epoch:7, train acc:0.938, test acc:0.924 ===\n",
      "train loss:0.1627053446819039\n",
      "train loss:0.13421994049589211\n",
      "train loss:0.1706177797660292\n",
      "train loss:0.3512129645802956\n",
      "train loss:0.1288491754641801\n",
      "train loss:0.06804599846464975\n",
      "train loss:0.16808919234623187\n",
      "train loss:0.14797595709888134\n",
      "train loss:0.1483035811230764\n",
      "train loss:0.14526501203502812\n",
      "train loss:0.13356487717915652\n",
      "train loss:0.12902465852239547\n",
      "train loss:0.2811482832101954\n",
      "train loss:0.13368333750346026\n",
      "train loss:0.21231018873518948\n",
      "train loss:0.08077094979929651\n",
      "train loss:0.1190278907869345\n",
      "train loss:0.14226756397309948\n",
      "train loss:0.19452795152788813\n",
      "train loss:0.12161567450551004\n",
      "train loss:0.14081532713439077\n",
      "train loss:0.10958397180658898\n",
      "train loss:0.10563842834236176\n",
      "train loss:0.05077167810706609\n",
      "train loss:0.08646415025390805\n",
      "train loss:0.11483972005740968\n",
      "train loss:0.13154108597483483\n",
      "train loss:0.16429117467514193\n",
      "train loss:0.08909482789613576\n",
      "train loss:0.171072293533386\n",
      "train loss:0.2028517829320399\n",
      "train loss:0.27986291167410604\n",
      "train loss:0.11002935182603872\n",
      "train loss:0.2028044050443043\n",
      "train loss:0.1672442598853184\n",
      "train loss:0.1639105106307376\n",
      "train loss:0.13034552000409017\n",
      "train loss:0.13529397164470114\n",
      "train loss:0.1146326544228336\n",
      "train loss:0.09851043567782479\n",
      "train loss:0.149203516775628\n",
      "train loss:0.16512556220367652\n",
      "train loss:0.07164650007584313\n",
      "train loss:0.1198812753605803\n",
      "train loss:0.1656354171050639\n",
      "train loss:0.1386592567605631\n",
      "train loss:0.13059966444040197\n",
      "train loss:0.10074093270935211\n",
      "train loss:0.24796820478107728\n",
      "train loss:0.09886913100155052\n",
      "=== epoch:8, train acc:0.953, test acc:0.922 ===\n",
      "train loss:0.1815643041796956\n",
      "train loss:0.18584943764483977\n",
      "train loss:0.15184242445011806\n",
      "train loss:0.1928377526873561\n",
      "train loss:0.16813481103595168\n",
      "train loss:0.11666418529154925\n",
      "train loss:0.05684453740647981\n",
      "train loss:0.1679127050822374\n",
      "train loss:0.127229199286333\n",
      "train loss:0.18478016168367542\n",
      "train loss:0.17981251132639056\n",
      "train loss:0.08874421001345553\n",
      "train loss:0.1566690570671036\n",
      "train loss:0.16341546467605397\n",
      "train loss:0.12564155308217984\n",
      "train loss:0.10963830724485142\n",
      "train loss:0.16480670015807597\n",
      "train loss:0.14063764287127398\n",
      "train loss:0.09126862857394044\n",
      "train loss:0.08306513254863537\n",
      "train loss:0.08225681684165069\n",
      "train loss:0.21566639914818408\n",
      "train loss:0.1304690729183085\n",
      "train loss:0.11243335426165645\n",
      "train loss:0.15099299563836532\n",
      "train loss:0.11833177490011869\n",
      "train loss:0.17645956596825482\n",
      "train loss:0.22788232504156336\n",
      "train loss:0.05845072308320968\n",
      "train loss:0.10770980955066331\n",
      "train loss:0.09223561465494062\n",
      "train loss:0.08582282872644154\n",
      "train loss:0.14352267374183472\n",
      "train loss:0.14872563172432574\n",
      "train loss:0.09118183399794912\n",
      "train loss:0.09930076838453897\n",
      "train loss:0.15231028261522261\n",
      "train loss:0.06813214696734585\n",
      "train loss:0.12215444262002695\n",
      "train loss:0.09911284799512529\n",
      "train loss:0.1784322739286812\n",
      "train loss:0.09236662368529724\n",
      "train loss:0.11127745576021585\n",
      "train loss:0.06116137469216978\n",
      "train loss:0.12443225896091556\n",
      "train loss:0.21968014762046517\n",
      "train loss:0.20390152702619135\n",
      "train loss:0.12984263522566927\n",
      "train loss:0.13815214311343546\n",
      "train loss:0.13365925295728856\n",
      "=== epoch:9, train acc:0.951, test acc:0.927 ===\n",
      "train loss:0.07635678928136516\n",
      "train loss:0.08812046844848226\n",
      "train loss:0.045334176743729415\n",
      "train loss:0.09900631764430509\n",
      "train loss:0.06701411481136856\n",
      "train loss:0.12749846264981526\n",
      "train loss:0.182336533530157\n",
      "train loss:0.1092976908825686\n",
      "train loss:0.060718398154744106\n",
      "train loss:0.05325600899060191\n",
      "train loss:0.12107715425301901\n",
      "train loss:0.10624258099995872\n",
      "train loss:0.08433914502609043\n",
      "train loss:0.04195841813116967\n",
      "train loss:0.0541416528632329\n",
      "train loss:0.0708087023633488\n",
      "train loss:0.09835195262111668\n",
      "train loss:0.1463370700548606\n",
      "train loss:0.12797215552714655\n",
      "train loss:0.08808708599864186\n",
      "train loss:0.15825864952816251\n",
      "train loss:0.1758316379253319\n",
      "train loss:0.07988875504505871\n",
      "train loss:0.16023877297184883\n",
      "train loss:0.13265036648910997\n",
      "train loss:0.16794303944766262\n",
      "train loss:0.08667107781471252\n",
      "train loss:0.07490472703030454\n",
      "train loss:0.03705244504956053\n",
      "train loss:0.08291795564162721\n",
      "train loss:0.0873227295042187\n",
      "train loss:0.16719731573905694\n",
      "train loss:0.13089442699051038\n",
      "train loss:0.05495193251422225\n",
      "train loss:0.10012194348873173\n",
      "train loss:0.07430601722166888\n",
      "train loss:0.07320695195850858\n",
      "train loss:0.14396062056524764\n",
      "train loss:0.04917426390056032\n",
      "train loss:0.081940684081192\n",
      "train loss:0.07536112913543329\n",
      "train loss:0.0656064932187166\n",
      "train loss:0.07252431653530088\n",
      "train loss:0.04443079429178904\n",
      "train loss:0.08310696321911144\n",
      "train loss:0.04997008166573244\n",
      "train loss:0.1318463341885144\n",
      "train loss:0.17639442536188146\n",
      "train loss:0.1311034775954657\n",
      "train loss:0.1043549903763851\n",
      "=== epoch:10, train acc:0.963, test acc:0.936 ===\n",
      "train loss:0.057033330171649806\n",
      "train loss:0.09794412879619933\n",
      "train loss:0.051094178415653536\n",
      "train loss:0.06319020113912585\n",
      "train loss:0.086585603745637\n",
      "train loss:0.05593462035262322\n",
      "train loss:0.13760978982894934\n",
      "train loss:0.07073934158374318\n",
      "train loss:0.02635242885738499\n",
      "train loss:0.03854520129254834\n",
      "train loss:0.0732824490352729\n",
      "train loss:0.05384903272726439\n",
      "train loss:0.13873601168080507\n",
      "train loss:0.10730795194959633\n",
      "train loss:0.09063193626720256\n",
      "train loss:0.1390593221034853\n",
      "train loss:0.13257752938379055\n",
      "train loss:0.09550550802560018\n",
      "train loss:0.03632566471765368\n",
      "train loss:0.1059486929797027\n",
      "train loss:0.052595423321132914\n",
      "train loss:0.0727066888819568\n",
      "train loss:0.08243663201605457\n",
      "train loss:0.06991679914504505\n",
      "train loss:0.10583387940430267\n",
      "train loss:0.09407213130463048\n",
      "train loss:0.11313074860709244\n",
      "train loss:0.08587597947579022\n",
      "train loss:0.0763139853949908\n",
      "train loss:0.06653942003694048\n",
      "train loss:0.03201163584625849\n",
      "train loss:0.056265305204030856\n",
      "train loss:0.1299724760203271\n",
      "train loss:0.11893980191868027\n",
      "train loss:0.10646127072119503\n",
      "train loss:0.0888695315951344\n",
      "train loss:0.051815600632324585\n",
      "train loss:0.16560156190778538\n",
      "train loss:0.1343445382640476\n",
      "train loss:0.1667029840727299\n",
      "train loss:0.13607432126897015\n",
      "train loss:0.10266820056468604\n",
      "train loss:0.1203673765713674\n",
      "train loss:0.04454998692752024\n",
      "train loss:0.10918406828201936\n",
      "train loss:0.12661706029566366\n",
      "train loss:0.11459825757615771\n",
      "train loss:0.11119069611255755\n",
      "train loss:0.04455641986818593\n",
      "train loss:0.12943722929215362\n",
      "=== epoch:11, train acc:0.965, test acc:0.944 ===\n",
      "train loss:0.14160749797412855\n",
      "train loss:0.06444007828135304\n",
      "train loss:0.09812614874828293\n",
      "train loss:0.07816872391272064\n",
      "train loss:0.1373494085607855\n",
      "train loss:0.03138222753862603\n",
      "train loss:0.06050631493636864\n",
      "train loss:0.05779860053576218\n",
      "train loss:0.12891756748052227\n",
      "train loss:0.06640048322309369\n",
      "train loss:0.12101485647202165\n",
      "train loss:0.050559822440824694\n",
      "train loss:0.09484792697257338\n",
      "train loss:0.03451624761027297\n",
      "train loss:0.056233042384284226\n",
      "train loss:0.06755110576797882\n",
      "train loss:0.10892866269492776\n",
      "train loss:0.15020338384177534\n",
      "train loss:0.10417168343535183\n",
      "train loss:0.06223333606554277\n",
      "train loss:0.05603618301756625\n",
      "train loss:0.04467869246992973\n",
      "train loss:0.070200846707094\n",
      "train loss:0.036883453390533616\n",
      "train loss:0.058939825182651624\n",
      "train loss:0.14042092157400826\n",
      "train loss:0.04212496152358021\n",
      "train loss:0.14950966116601058\n",
      "train loss:0.05182222438716895\n",
      "train loss:0.07974834465825653\n",
      "train loss:0.09566097091978099\n",
      "train loss:0.0760772795152176\n",
      "train loss:0.07174076179482555\n",
      "train loss:0.12089507539694327\n",
      "train loss:0.09627661895869442\n",
      "train loss:0.06632699122765595\n",
      "train loss:0.02306025458079771\n",
      "train loss:0.03812549256556283\n",
      "train loss:0.08579982877200885\n",
      "train loss:0.06353936867093085\n",
      "train loss:0.06574292999367376\n",
      "train loss:0.14818488905640184\n",
      "train loss:0.06857985943609746\n",
      "train loss:0.07578854296563288\n",
      "train loss:0.036385503163670375\n",
      "train loss:0.034448050924142826\n",
      "train loss:0.06571612184771417\n",
      "train loss:0.10824398894395487\n",
      "train loss:0.057288032539192936\n",
      "train loss:0.08057107163567646\n",
      "=== epoch:12, train acc:0.972, test acc:0.952 ===\n",
      "train loss:0.019848089461436097\n",
      "train loss:0.0553327898342824\n",
      "train loss:0.12421590110295544\n",
      "train loss:0.0657736043751524\n",
      "train loss:0.04193910445502546\n",
      "train loss:0.0859268673462398\n",
      "train loss:0.06576743909707006\n",
      "train loss:0.04499324990955625\n",
      "train loss:0.11630333414240297\n",
      "train loss:0.04250219692401849\n",
      "train loss:0.10977724227280468\n",
      "train loss:0.034874418048554345\n",
      "train loss:0.09651447188480197\n",
      "train loss:0.04541712405463387\n",
      "train loss:0.03152177195363833\n",
      "train loss:0.08184166990194588\n",
      "train loss:0.07354870808695953\n",
      "train loss:0.10169799606435617\n",
      "train loss:0.06110540436547941\n",
      "train loss:0.055680141280034554\n",
      "train loss:0.08440907540498711\n",
      "train loss:0.056198505097648316\n",
      "train loss:0.11477285269644522\n",
      "train loss:0.045334561152204106\n",
      "train loss:0.044695026603448006\n",
      "train loss:0.01912202376079507\n",
      "train loss:0.05985462623550244\n",
      "train loss:0.03010755856556416\n",
      "train loss:0.07004449593695346\n",
      "train loss:0.03344710471239051\n",
      "train loss:0.04087837731925739\n",
      "train loss:0.046874841414016565\n",
      "train loss:0.08167733735434712\n",
      "train loss:0.029266909570459937\n",
      "train loss:0.10179128990951913\n",
      "train loss:0.046532927942940026\n",
      "train loss:0.12944164726325927\n",
      "train loss:0.03292755796077594\n",
      "train loss:0.049428222867289415\n",
      "train loss:0.03933969206443535\n",
      "train loss:0.051701191202978655\n",
      "train loss:0.04070762890140096\n",
      "train loss:0.019499221180527698\n",
      "train loss:0.07685939449318424\n",
      "train loss:0.06268821371925275\n",
      "train loss:0.05862912500288716\n",
      "train loss:0.046268841246293685\n",
      "train loss:0.09324367437246042\n",
      "train loss:0.045527545435229674\n",
      "train loss:0.10988899091282944\n",
      "=== epoch:13, train acc:0.971, test acc:0.938 ===\n",
      "train loss:0.12300037023295171\n",
      "train loss:0.05340737785175933\n",
      "train loss:0.08674448189674405\n",
      "train loss:0.056800849605714306\n",
      "train loss:0.019028864728008032\n",
      "train loss:0.05685907190247222\n",
      "train loss:0.0585415633052997\n",
      "train loss:0.03990308956093435\n",
      "train loss:0.04691680442074638\n",
      "train loss:0.1009100561605575\n",
      "train loss:0.12507341381189474\n",
      "train loss:0.13131479829588746\n",
      "train loss:0.04647757064595138\n",
      "train loss:0.06283568648000597\n",
      "train loss:0.026841918615433343\n",
      "train loss:0.038918212312090125\n",
      "train loss:0.027300782142194228\n",
      "train loss:0.16706147466434637\n",
      "train loss:0.04703463366706975\n",
      "train loss:0.060607741406943284\n",
      "train loss:0.07929350201716971\n",
      "train loss:0.05871759602108078\n",
      "train loss:0.03770677827731979\n",
      "train loss:0.010267651171407067\n",
      "train loss:0.02258008409703325\n",
      "train loss:0.05047843318332737\n",
      "train loss:0.04242843779163766\n",
      "train loss:0.04720093726383425\n",
      "train loss:0.02459295047367099\n",
      "train loss:0.05232396358358708\n",
      "train loss:0.02413056552550492\n",
      "train loss:0.012268641319493172\n",
      "train loss:0.039208090786390766\n",
      "train loss:0.03632290706778273\n",
      "train loss:0.07166622764894204\n",
      "train loss:0.027723894295130606\n",
      "train loss:0.08012849036471315\n",
      "train loss:0.06024740945689455\n",
      "train loss:0.05132122150777949\n",
      "train loss:0.07058607961142228\n",
      "train loss:0.04513807389694904\n",
      "train loss:0.05351078504974821\n",
      "train loss:0.06456794844921783\n",
      "train loss:0.03753533217228695\n",
      "train loss:0.0475463271779957\n",
      "train loss:0.03135617327333548\n",
      "train loss:0.052111082925684185\n",
      "train loss:0.04923316659334767\n",
      "train loss:0.05630579051751275\n",
      "train loss:0.047371790872092784\n",
      "=== epoch:14, train acc:0.979, test acc:0.949 ===\n",
      "train loss:0.046266336444246374\n",
      "train loss:0.022214994980547575\n",
      "train loss:0.009017345202268023\n",
      "train loss:0.0421575614270032\n",
      "train loss:0.0561848031875483\n",
      "train loss:0.043627561677472906\n",
      "train loss:0.03617850371177027\n",
      "train loss:0.057909929633301944\n",
      "train loss:0.040763659893359276\n",
      "train loss:0.04983115201917051\n",
      "train loss:0.049297450567366174\n",
      "train loss:0.08705184700792486\n",
      "train loss:0.025033274805330023\n",
      "train loss:0.042973082643578915\n",
      "train loss:0.038448400977010556\n",
      "train loss:0.08051520359837866\n",
      "train loss:0.03412158623981564\n",
      "train loss:0.02853150011663448\n",
      "train loss:0.05448958913460739\n",
      "train loss:0.0297483713434786\n",
      "train loss:0.02816006814990863\n",
      "train loss:0.03270748184992988\n",
      "train loss:0.01620048044590833\n",
      "train loss:0.040547334523469925\n",
      "train loss:0.01987050810962495\n",
      "train loss:0.07105213785217587\n",
      "train loss:0.04243483120529141\n",
      "train loss:0.017982089850998938\n",
      "train loss:0.04222401164796586\n",
      "train loss:0.028913806680086047\n",
      "train loss:0.05628944951471543\n",
      "train loss:0.028177469438933088\n",
      "train loss:0.02036930920207125\n",
      "train loss:0.03968775371302558\n",
      "train loss:0.036099170737313856\n",
      "train loss:0.05486831214222569\n",
      "train loss:0.016073626257483386\n",
      "train loss:0.02414224280438845\n",
      "train loss:0.08335743378613389\n",
      "train loss:0.017171046346746797\n",
      "train loss:0.018599014903800934\n",
      "train loss:0.039724572629022244\n",
      "train loss:0.026449636696245813\n",
      "train loss:0.03602731782840873\n",
      "train loss:0.02315473580572025\n",
      "train loss:0.06511081930596208\n",
      "train loss:0.017274742486958084\n",
      "train loss:0.03814788234373491\n",
      "train loss:0.01531547987722797\n",
      "train loss:0.017448191146285492\n",
      "=== epoch:15, train acc:0.983, test acc:0.957 ===\n",
      "train loss:0.060608374278272946\n",
      "train loss:0.09481668554744108\n",
      "train loss:0.05253083352346807\n",
      "train loss:0.052004757611495325\n",
      "train loss:0.019625232508589347\n",
      "train loss:0.08916915364220124\n",
      "train loss:0.027041658255579602\n",
      "train loss:0.06023793721104303\n",
      "train loss:0.02904587782400836\n",
      "train loss:0.027436878242304834\n",
      "train loss:0.05661165527104821\n",
      "train loss:0.009177056574732386\n",
      "train loss:0.01463075408690309\n",
      "train loss:0.02426349210792619\n",
      "train loss:0.05412652336330387\n",
      "train loss:0.03381120673624237\n",
      "train loss:0.014239813879179783\n",
      "train loss:0.010123321870734153\n",
      "train loss:0.06795280446356318\n",
      "train loss:0.02961049660491853\n",
      "train loss:0.040596039252576925\n",
      "train loss:0.06453153914281795\n",
      "train loss:0.035096367203535504\n",
      "train loss:0.014836114603247484\n",
      "train loss:0.03661848015664986\n",
      "train loss:0.016668089867743666\n",
      "train loss:0.07589895082596679\n",
      "train loss:0.07427186038249055\n",
      "train loss:0.08132530863094523\n",
      "train loss:0.012103061446031727\n",
      "train loss:0.03388892040266539\n",
      "train loss:0.058705079873496775\n",
      "train loss:0.07922038660600264\n",
      "train loss:0.020598270519858422\n",
      "train loss:0.01798249203019144\n",
      "train loss:0.030401929081478492\n",
      "train loss:0.03055272233495885\n",
      "train loss:0.0766189394340652\n",
      "train loss:0.046082192601483066\n",
      "train loss:0.031887533300998076\n",
      "train loss:0.021206961392655453\n",
      "train loss:0.076408314849385\n",
      "train loss:0.023821759181707064\n",
      "train loss:0.022409461630369974\n",
      "train loss:0.047462721924009584\n",
      "train loss:0.05995972113087767\n",
      "train loss:0.01807909210963062\n",
      "train loss:0.019036104829330738\n",
      "train loss:0.0059607831830288385\n",
      "train loss:0.046631094360769766\n",
      "=== epoch:16, train acc:0.975, test acc:0.948 ===\n",
      "train loss:0.084979945446565\n",
      "train loss:0.02575251609769543\n",
      "train loss:0.018734705228801506\n",
      "train loss:0.02302112626383449\n",
      "train loss:0.021328606899788074\n",
      "train loss:0.023267738335267074\n",
      "train loss:0.03727960359327196\n",
      "train loss:0.07325017215408947\n",
      "train loss:0.011710260396543548\n",
      "train loss:0.04216745430749904\n",
      "train loss:0.037441707937041234\n",
      "train loss:0.027890055707689352\n",
      "train loss:0.037240105651190875\n",
      "train loss:0.01625083171021051\n",
      "train loss:0.0277874881848387\n",
      "train loss:0.033567231633387394\n",
      "train loss:0.06109942369910566\n",
      "train loss:0.008489904670019487\n",
      "train loss:0.03815841005768955\n",
      "train loss:0.013734715433806964\n",
      "train loss:0.037493657672913024\n",
      "train loss:0.04534512896016779\n",
      "train loss:0.02250069922402471\n",
      "train loss:0.01489029956424591\n",
      "train loss:0.015172500826430185\n",
      "train loss:0.021690323162643566\n",
      "train loss:0.01609465392002652\n",
      "train loss:0.015642068196960093\n",
      "train loss:0.020113522503030352\n",
      "train loss:0.018881966891121225\n",
      "train loss:0.03147014198591516\n",
      "train loss:0.060396275832426946\n",
      "train loss:0.028659471198851184\n",
      "train loss:0.03168596637498409\n",
      "train loss:0.016596898829620354\n",
      "train loss:0.043556520845598426\n",
      "train loss:0.017966545869843226\n",
      "train loss:0.024902393149068756\n",
      "train loss:0.016920759346500856\n",
      "train loss:0.039905834652396006\n",
      "train loss:0.03071210376259057\n",
      "train loss:0.0306646522198111\n",
      "train loss:0.034099892832034254\n",
      "train loss:0.029190247330918975\n",
      "train loss:0.10150907971093875\n",
      "train loss:0.009064088558700998\n",
      "train loss:0.048683282138623996\n",
      "train loss:0.01240594118860846\n",
      "train loss:0.014295735058941574\n",
      "train loss:0.017782918382999165\n",
      "=== epoch:17, train acc:0.985, test acc:0.949 ===\n",
      "train loss:0.01982011261071358\n",
      "train loss:0.03686556935518688\n",
      "train loss:0.014631614806920108\n",
      "train loss:0.03329201890262827\n",
      "train loss:0.02641783588308323\n",
      "train loss:0.015402157723172698\n",
      "train loss:0.029758691380311852\n",
      "train loss:0.021365066510163188\n",
      "train loss:0.05779636107084803\n",
      "train loss:0.010738774354962843\n",
      "train loss:0.02929482881513461\n",
      "train loss:0.04153156051997822\n",
      "train loss:0.012878946877869264\n",
      "train loss:0.026641711315029255\n",
      "train loss:0.04587278868339885\n",
      "train loss:0.031105466981498645\n",
      "train loss:0.018321440891992863\n",
      "train loss:0.012347426795976477\n",
      "train loss:0.05297608065563067\n",
      "train loss:0.02409671499505515\n",
      "train loss:0.015915488068425004\n",
      "train loss:0.015675705013200578\n",
      "train loss:0.024631665448867142\n",
      "train loss:0.007328441023377351\n",
      "train loss:0.0210828870557541\n",
      "train loss:0.0223890445143197\n",
      "train loss:0.02667771785067035\n",
      "train loss:0.010951397770324398\n",
      "train loss:0.02814260330414582\n",
      "train loss:0.016465308222605585\n",
      "train loss:0.030599390958214873\n",
      "train loss:0.03217941304222971\n",
      "train loss:0.016703590396727877\n",
      "train loss:0.021863855800041736\n",
      "train loss:0.031085824576447676\n",
      "train loss:0.02386550129646384\n",
      "train loss:0.07208874478884413\n",
      "train loss:0.0184852547643687\n",
      "train loss:0.006884488398962212\n",
      "train loss:0.025284722744509765\n",
      "train loss:0.017988791399587575\n",
      "train loss:0.045522454720565625\n",
      "train loss:0.0503755815534303\n",
      "train loss:0.02477528465384947\n",
      "train loss:0.020443277480298394\n",
      "train loss:0.03007360517134944\n",
      "train loss:0.05293504596259769\n",
      "train loss:0.015725074496759753\n",
      "train loss:0.04601858748568246\n",
      "train loss:0.039676200043085516\n",
      "=== epoch:18, train acc:0.986, test acc:0.953 ===\n",
      "train loss:0.009401091864659684\n",
      "train loss:0.019024699631816525\n",
      "train loss:0.037138142069707565\n",
      "train loss:0.022164057577570256\n",
      "train loss:0.02725990648367593\n",
      "train loss:0.01582156771212662\n",
      "train loss:0.016614779997038073\n",
      "train loss:0.02316784516508856\n",
      "train loss:0.02440880710528905\n",
      "train loss:0.015305384888298048\n",
      "train loss:0.03763202875596844\n",
      "train loss:0.036901043332639445\n",
      "train loss:0.022529743128131396\n",
      "train loss:0.010759504761669566\n",
      "train loss:0.017610696955355422\n",
      "train loss:0.039180034912756775\n",
      "train loss:0.029832608369785633\n",
      "train loss:0.040402650128326514\n",
      "train loss:0.04399029883153125\n",
      "train loss:0.04522326837480702\n",
      "train loss:0.051104330115532864\n",
      "train loss:0.01722842504011179\n",
      "train loss:0.05376196038561824\n",
      "train loss:0.009991916030478177\n",
      "train loss:0.010498405795215857\n",
      "train loss:0.008627693877500594\n",
      "train loss:0.0183587444906198\n",
      "train loss:0.008872295817218361\n",
      "train loss:0.007036425603861556\n",
      "train loss:0.025365051957653026\n",
      "train loss:0.022231163677555177\n",
      "train loss:0.04698953577851205\n",
      "train loss:0.01425286533251405\n",
      "train loss:0.02309327855874045\n",
      "train loss:0.025514881204961985\n",
      "train loss:0.03841838623126652\n",
      "train loss:0.03408565326372493\n",
      "train loss:0.027194775436821694\n",
      "train loss:0.045544234236994746\n",
      "train loss:0.01588536737271644\n",
      "train loss:0.018942963525951918\n",
      "train loss:0.023183935394807908\n",
      "train loss:0.01264227878703892\n",
      "train loss:0.03122361201069977\n",
      "train loss:0.0194361628515869\n",
      "train loss:0.006153936030188895\n",
      "train loss:0.0172473651358197\n",
      "train loss:0.010280374298680244\n",
      "train loss:0.034380910209249056\n",
      "train loss:0.011094480277957558\n",
      "=== epoch:19, train acc:0.99, test acc:0.962 ===\n",
      "train loss:0.013977285182902565\n",
      "train loss:0.013576271988172349\n",
      "train loss:0.01072856308115751\n",
      "train loss:0.02161947484629646\n",
      "train loss:0.018397227079062748\n",
      "train loss:0.008586383299429299\n",
      "train loss:0.015271649536613054\n",
      "train loss:0.033440914522608306\n",
      "train loss:0.01636348430646182\n",
      "train loss:0.009113772351015592\n",
      "train loss:0.02115335739416702\n",
      "train loss:0.0072818046095098465\n",
      "train loss:0.03102604113022039\n",
      "train loss:0.016157750179675162\n",
      "train loss:0.013990595412049936\n",
      "train loss:0.004361877674150244\n",
      "train loss:0.021623105421659336\n",
      "train loss:0.00847594043709686\n",
      "train loss:0.008937493493801463\n",
      "train loss:0.01859783525273532\n",
      "train loss:0.009976506767027908\n",
      "train loss:0.024582951549709565\n",
      "train loss:0.018842749024143235\n",
      "train loss:0.014351683841817151\n",
      "train loss:0.015623502291389863\n",
      "train loss:0.006583719137238542\n",
      "train loss:0.009690936857713505\n",
      "train loss:0.011730181434255981\n",
      "train loss:0.017323479047164324\n",
      "train loss:0.026350665765189296\n",
      "train loss:0.015750462247100253\n",
      "train loss:0.08132960172040336\n",
      "train loss:0.012699194929870294\n",
      "train loss:0.017498991261426903\n",
      "train loss:0.02449135352297546\n",
      "train loss:0.04477129738784098\n",
      "train loss:0.017195138795836513\n",
      "train loss:0.014165605210538565\n",
      "train loss:0.01146062182823216\n",
      "train loss:0.0077594699110540314\n",
      "train loss:0.025794981459477312\n",
      "train loss:0.021045896948628604\n",
      "train loss:0.012711613613722217\n",
      "train loss:0.015384285702640863\n",
      "train loss:0.017321687946464725\n",
      "train loss:0.015810934069817536\n",
      "train loss:0.00724964628748735\n",
      "train loss:0.04526315427923762\n",
      "train loss:0.03222199392626536\n",
      "train loss:0.003927217233124582\n",
      "=== epoch:20, train acc:0.995, test acc:0.957 ===\n",
      "train loss:0.010366740372920099\n",
      "train loss:0.030714746174374186\n",
      "train loss:0.009057004971833404\n",
      "train loss:0.007738204635858146\n",
      "train loss:0.013326309703304347\n",
      "train loss:0.011751764467752895\n",
      "train loss:0.006947743705037737\n",
      "train loss:0.009134702397382864\n",
      "train loss:0.03329606038887494\n",
      "train loss:0.011354002726062995\n",
      "train loss:0.00786505784531432\n",
      "train loss:0.010235686501218191\n",
      "train loss:0.014900894216479964\n",
      "train loss:0.020504012540169238\n",
      "train loss:0.032883950653197866\n",
      "train loss:0.00487971196346858\n",
      "train loss:0.010591887135763787\n",
      "train loss:0.01854508696905358\n",
      "train loss:0.006520880036384973\n",
      "train loss:0.011316526567005864\n",
      "train loss:0.015387258018075098\n",
      "train loss:0.012595157027537576\n",
      "train loss:0.006659382936555627\n",
      "train loss:0.006377310831914894\n",
      "train loss:0.008413091362834784\n",
      "train loss:0.01127944664695279\n",
      "train loss:0.010762258617396096\n",
      "train loss:0.010887020680076173\n",
      "train loss:0.010962982629368014\n",
      "train loss:0.005792455510384883\n",
      "train loss:0.006953735093651694\n",
      "train loss:0.0101507057358837\n",
      "train loss:0.008533436664978524\n",
      "train loss:0.009434425331798675\n",
      "train loss:0.011243207923324753\n",
      "train loss:0.011226562894963925\n",
      "train loss:0.013655492052728176\n",
      "train loss:0.06232785031891696\n",
      "train loss:0.01029607109775191\n",
      "train loss:0.040024797405592866\n",
      "train loss:0.01737304807462983\n",
      "train loss:0.007486618931982368\n",
      "train loss:0.013437371868391541\n",
      "train loss:0.01021693091357043\n",
      "train loss:0.025299723786242113\n",
      "train loss:0.011692642538128756\n",
      "train loss:0.015328620409516077\n",
      "train loss:0.0399797491657778\n",
      "train loss:0.007380528561251111\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.961\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
